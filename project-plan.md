# ASDM Buffelgrass Mapping - Project Plan (Summary)

> **Note:** This is a high-level summary automatically derived from [`project-plan-detailed.md`](project-plan-detailed.md).  
> **Do not edit this file directly.** All detailed specifications, code snippets, and implementation details are maintained in the detailed plan.  
> Last updated: October 29, 2025

---

## Project Overview

Machine learning pipeline for detecting and mapping buffelgrass (*Pennisetum ciliare*) in southern Arizona using aerial imagery from Air Data Solutions and Intel's Geti platform.

**Goal:** Produce actionable buffelgrass distribution maps to support invasive species management efforts in the Sonoran Desert region.

**Approach:** Hybrid local/cloud architecture
- **Local (Mac M2 Max):** Annotation and project management using k3d
- **Cloud (GCP GPU VM):** Model training only, triggered via SSH + SDK
- **Workflow:** Annotate locally â†’ Export â†’ Cloud training â†’ Import model back

---

## Implementation Phases

### Phase 1: Data Preprocessing (2-3 days)

**Objective:** Convert source imagery to cloud-optimized format

**Key Steps:**
1. Download Tumamoc source JPEG files from Dropbox
2. Convert to Cloud-Optimized GeoTIFF (COG) format
   - Use `rio-cogeo` with JPEG compression (quality 90)
   - 512x512 internal tiling, 5 overview levels
3. Upload COGs to Google Cloud Storage bucket
4. Verify windowed reads work from cloud

**Why COG?** No patent concerns, better performance, industry standard for ML workflows

**Deliverables:**
- All source imagery in COG format
- COGs uploaded to GCS
- Validation complete

---

### Phase 2: Training Chip Generation (1-2 days)

**Objective:** Generate diverse training image chips from source COGs

**Approach:**
- Random selection of source images
- Random windowed areas within each image
- Chip size: 1024x1024 (to be confirmed with Geti docs)
- Filter low-information chips (water, uniform areas)
- Target: 50-100 initial chips

**Key Steps:**
1. Determine optimal chip size for segmentation
2. Implement extraction with quality control
3. Visual inspection for diversity
4. Download chips to local storage

**Deliverables:**
- Chip generation script
- 50-100 diverse training chips
- Chip metadata (source, coordinates, date)

---

### Phase 3: Infrastructure Setup (1 day)

**Objective:** Deploy local Geti for annotation, prepare cloud VM for GPU training

**Architecture: Hybrid Approach**
- **Local:** CPU-only Geti for annotation (no GPU needed, $0 cost)
- **Cloud:** GPU VM for training only (on-demand, minimize costs)
- **Data Transfer:** Via GCS and Geti SDK export/import

#### 3.1: Local Geti Setup (Annotation Environment)

**Steps:**
1. Install k3d on Mac
2. Create k3d cluster with persistent volume mapping
3. Deploy Geti (CPU-only, `gpu_support: false`)
4. Access at `http://localhost:8080`

**Key Point:** Web UI only for annotation, not training

#### 3.2: Cloud VM Setup (Training Environment)

**Simplified approach:** SSH-only access, no public web UI needed

**VM Specs:**
- Machine: n1-standard-8 (8 vCPUs, 30 GB RAM)
- GPU: NVIDIA Tesla T4 (16GB VRAM)
- Disk: 200GB standard
- Cost: ~$0.60/hour
- **No firewall rules needed** - only SSH access

**Security Benefits:**
- No public web UI exposure
- SSH-only access (GCP default)
- Can use internal IP only (optional)
- Simpler configuration

**Training Method:** All operations via Geti SDK over SSH
- No browser required
- Fully scriptable
- Can trigger remotely from Mac

**Reference:** See [`docs/remote-training-via-sdk.md`](docs/remote-training-via-sdk.md) for detailed SSH + SDK workflow

#### 3.3: Data Transfer Workflow

**Pattern:** Local â†’ GCS â†’ Cloud â†’ Train â†’ GCS â†’ Local

1. Export project from local Geti (via SDK)
2. Upload export to GCS
3. Download on cloud VM and import (via SDK)
4. Train on GPU (via SDK - no web UI)
5. Export trained model (via SDK)
6. Upload to GCS
7. Download to Mac and import to local Geti (optional)

**Deliverables:**
- k3d cluster running locally
- Cloud VM created (but stopped when not training)
- Data transfer scripts

---

### Phase 4: Project Setup & Image Loading (1 day)

**Objective:** Create Geti project and load training chips

**Image Loading Strategy:** Direct upload to local Geti via SDK
- Upload chips from Mac to local Geti instance
- Fast local access during annotation
- Annotations stored in persistent volume (`~/geti-data`)

**Steps:**
1. Create segmentation project via SDK
2. Configure project (labels: buffelgrass, background)
3. Upload training chips
4. Verify project configuration

**Deliverables:**
- Geti project created locally
- Training chips uploaded
- SDK scripts for project management

---

### Phase 5: Annotation & Model Training (1-2 weeks)

**Objective:** Annotate locally, train on cloud GPU

#### Annotation Phase (Local, ~10-30 hours)

**Process:**
1. Annotate 20-30 images using local Geti web UI
2. Leverage smart annotation tools (SAM, visual prompting)
3. Label buffelgrass vs. background
4. Time: ~30-60 minutes per image with AI assistance
5. Export annotated project via SDK

**Key Decisions:**
- Binary segmentation (buffelgrass vs. background)
- Annotation quality guidelines
- Edge case documentation

#### Training Phase (Cloud GPU)

**Workflow:**
1. Start cloud VM (if stopped)
2. Upload project export to GCS
3. SSH to VM and run training script
4. Script handles: import â†’ train â†’ monitor â†’ export â†’ upload to GCS
5. Stop VM to save costs

**Training Details:**
- Method: SDK-based (no web UI)
- Time: 30-60 minutes with GPU (vs. 5-10 hours on CPU)
- Architecture: RTMDet or MaskRCNN (Geti default)
- Monitoring: Via SDK logging

**Cost per iteration:** ~$0.50-1.00 (30-60 minutes of VM time)

**Iteration Cycle:**
1. Train on cloud â†’ Export model â†’ Download
2. Review results locally
3. Annotate more images based on errors
4. Repeat 3-5 times

**Deliverables:**
- 50-100 annotated images
- Trained segmentation model
- Model performance metrics
- Annotation guidelines document

---

### Phase 6: Model Inspection & Iteration (3-5 days)

**Objective:** Validate and refine model performance

**Activities:**
1. Apply model to unannotated chips (locally or cloud)
2. Visual inspection of predictions
3. Error analysis (false positives/negatives)
4. Targeted improvement (annotate problem areas)
5. Compare model versions

**Deliverables:**
- Error analysis report
- Final model selection with justification
- Model performance documentation

---

### Phase 7: Full Imagery Inference (2-4 days)

**Objective:** Apply trained model to all source COGs

**Approach:**
- Cloud-based inference using trained model
- Tile-based processing matching training chip size
- Handle tile overlaps for seamless output
- Output as COG format with confidence scores

**Workflow:**
1. Deploy model for inference (cloud VM or standalone)
2. Read COGs from GCS using windowed reads
3. Process in tiles, stitch predictions
4. Save predictions as COG (binary + confidence band)
5. Upload prediction COGs to GCS

**Deliverables:**
- Inference pipeline script
- All source COGs processed
- Prediction COGs in GCS
- Inference performance metrics

---

### Phase 8: Mosaic Generation & Delivery (1 week)

**Objective:** Create final buffelgrass distribution map

**Workflow:**
1. Provide GCS access to Stephen (Air Data Solutions)
2. Stephen applies same mosaic config as original imagery
3. Inputs: Prediction COGs from Phase 7
4. Output: Seamless buffelgrass distribution mosaic

**Quality Control:**
- Visual inspection of final mosaic
- Check for artifacts at tile boundaries
- Verify spatial accuracy
- Ground-truth validation if possible

**Final Deliverables:**
- Final buffelgrass distribution map (GeoTIFF/COG)
- Confidence map
- Metadata and methodology documentation
- Web map visualization (optional)

---

## Technical Stack

**Infrastructure:**
- **Local:** k3d + Geti (CPU-only) + Mac M2 Max
- **Cloud:** GCP VM (n1-standard-8 + NVIDIA T4 GPU, on-demand)
- **Storage:** GCS buckets for data transfer and backup

**Key Technologies:**
- Geti SDK - Project and training management
- Rasterio/GDAL - Geospatial data handling
- rio-cogeo - COG conversion
- google-cloud-storage - Cloud data transfer
- k3d - Local Kubernetes
- k3s - Cloud Kubernetes

**Workflow Pattern:**
Local (annotate) â†’ Export â†’ Cloud (train) â†’ Export â†’ Local (iterate)

---

## Key Decisions

### âœ… Resolved Decisions

1. **Data Format:** COG (cloud-optimized GeoTIFF)
   - Rationale: No patent concerns, better performance, industry standard

2. **GPU Strategy:** Hybrid local + cloud
   - Local: Annotation only (no GPU needed)
   - Cloud: Training only (on-demand GPU)
   - Rationale: Minimize cloud costs while leveraging GPU acceleration

3. **Training Access:** SSH + SDK only (no public web UI)
   - Rationale: Simpler security, fully scriptable, no firewall config

4. **Image Loading:** Direct SDK upload to local Geti
   - Rationale: Fast local access during annotation phase

### ðŸ”„ To Be Determined

1. **Chip size:** 1024x1024 (pending Geti docs confirmation)
2. **GCS bucket name:** TBD (e.g., `asdm-buffelgrass`)
3. **Annotation scope:** Binary or multi-class segmentation
4. **Validation strategy:** Ground-truth data availability

---

## Timeline & Costs

### Timeline (4-6 weeks)

| Phase | Duration | Where | GPU Hours |
|-------|----------|-------|-----------|
| 1. Data Preprocessing | 2-3 days | Mac | 0 |
| 2. Chip Generation | 1-2 days | Mac | 0 |
| 3. Infrastructure Setup | 1 day | Mac + GCP | 0 |
| 4. Project Setup | 1 day | Mac | 0 |
| 5. Annotation | 1-2 weeks | Mac | 0 |
| 6. Training Iteration 1 | 2-3 hours | Cloud | 1-2 |
| 7. Training Iterations 2-5 | 1 week | Mac â†’ Cloud | 8-15 |
| 8. Full Inference | 2-4 days | Cloud | 5-10 |
| 9. Mosaic & Delivery | 1 week | Mac + GCS | 0 |

**Total Time:** 4-6 weeks  
**Total GPU Hours:** 15-30 hours

### Cost Estimates

**Cloud VM Running:** ~$0.60/hour
- Annotation phase: **$0** (local only)
- Training iteration: **$0.50-1.00** (30-60 min)
- 5 training iterations: **$5-10**
- Full inference: **$5-10**

**Total Project Cost:** **$10-20**
- vs. $24-36 for always-on cloud
- vs. $0 GPU but 200+ CPU hours (impractical)

**Cost Optimization:**
- Create VM only when ready to train
- Stop VM between training sessions
- Delete VM when project complete

---

## Success Metrics

**Technical:**
- IoU (Intersection over Union) > 0.75
- Pixel accuracy > 90%
- F1-score > 0.80
- False positive rate < 10%

**Operational:**
- Processing time: < 1 hour per source image for inference
- Model training: < 1 week from annotation start to final model
- Total project timeline: 4-6 weeks

**Deliverable:**
- Final map in actionable format
- Complete documentation
- Reusable pipeline for future imagery

---

## Risk Assessment

**Technical Risks:**
- GCP VM setup complexity â†’ Detailed scripts, SSH-only simplifies
- SSH connectivity â†’ GCP built-in SSH, test early
- GPU OOM during training â†’ Monitor VRAM, adjust batch sizes
- SDK script failures â†’ Error handling and retry logic
- Model performance insufficient â†’ Active learning, more data

**Data Risks:**
- Training data not representative â†’ Diverse chip sampling
- Annotation quality inconsistent â†’ Guidelines, QC process
- Seasonal variation â†’ Multi-season training data

**Coordination Risks:**
- Delays in ADS mosaicing â†’ Early communication
- GCS access/permissions â†’ Set up and test early

**Cost Risks:**
- Forgetting to stop VM â†’ Calendar reminders
- Underestimating GPU hours â†’ Budget $50 buffer

---

## Next Steps

**Week 1 (Immediate):**
1. Set up GCP project and GCS buckets
2. Set up local k3d cluster on Mac
3. Deploy local Geti for annotation
4. Download Tumamoc source JPEGs from Dropbox
5. Implement COG conversion script

**Weeks 2-3 (Short-term):**
6. Generate training chips locally
7. Upload chips to local Geti
8. Begin annotation (target 20-30 images)
9. Create GCP VM with GPU when ready to train
10. Run first training iteration on cloud

**Weeks 3-4 (Medium-term):**
11. Export trained model, review results
12. Continue annotation iterations
13. Periodic training on cloud (stop VM between sessions)

**Weeks 4-6 (Final):**
14. Iterate on model performance
15. Full inference pipeline
16. Coordinate with ADS for final mosaicing
17. Deliver final products

---

## Documentation Structure

**This File:** High-level project overview and timeline  
**Detailed Plan:** [`project-plan-detailed.md`](project-plan-detailed.md) - Complete specifications, code, workflows  
**Technical Docs:** [`docs/`](docs/) directory
- `remote-training-via-sdk.md` - SSH + SDK training workflow (8000+ words)
- `hybrid-workflow.md` - Complete local + cloud workflow
- `cost-management.md` - VM lifecycle and optimization
- `annotation-guidelines.md` - To be created
- `model-training-log.md` - To be created

**Scripts:** [`scripts/`](scripts/) directory
- `train_remote.py` - Remote GPU training (SDK-based)
- `export_local_project.py` - Export from local Geti
- Additional processing scripts to be created

---

## Project Context

**Stakeholders:**
- Land managers and conservation organizations (map users)
- Air Data Solutions / Stephen Ambagis (imagery provider)
- ASDM project team (implementation)

**Background:**
Buffelgrass is an invasive species that increases fire frequency and intensity in the Sonoran Desert. Accurate mapping enables targeted management and eradication efforts.

**Innovation:**
- Hybrid local/cloud architecture minimizes costs
- SSH + SDK workflow simplifies security
- Active learning reduces annotation burden
- COG format enables efficient large-area processing

---

**For detailed implementation instructions, code snippets, and technical specifications, refer to [`project-plan-detailed.md`](project-plan-detailed.md).**

**Last Updated:** October 29, 2025  
**Version:** 1.0  
**Status:** Planning Phase

